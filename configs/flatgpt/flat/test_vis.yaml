save_dir: /vol/data/trainings/all_clean/vqflatgpt/qwen2p5_flat

model_name: FlatGPTEmbedsRVQ
loss_name: CrossEntropyWithCodes
model_config: configs/flatgpt/flat/model_chmix.yaml

datasplitter:
  dataset_class: ChunkDatasetReconstruction
  dataset_root:
    mous: /vol/data/datasets/mous/full/cleaned
    omega: /vol/data/datasets/omega/full/cleaned
    camcan: /vol/data/datasets/camcan/full/cleaned
  example_seconds: 61.44
  overlap_seconds: 30.0
  split_strategy: dataset
  heldout_dataset: mous
  refresh_cache: false  # need to refresh if any datasplitter args change
  cache_dir: /vol/data/datasets/all/cache/cleaned_61.44s_co-m

dataloader:
  batch_size: 16
  num_workers: 16
  prefetch_factor: 4
  pin_memory: true
  persistent_workers: true

eval_runner:
  enabled: true
  ckpt_path: /vol/data/trainings/all_clean/vqflatgpt/qwen2p5_flat/logs/version_2/checkpoints/last-checkpoint-epoch00008.ckpt
  output_dir: logs/version_2/evals/vis_60s_main
  max_batches: 10
  num_examples: 0
  metrics_split: test

  # STEP 1: Sample full sessions (concatenated chunks).
  example_sampler:
    seed: 42
    split: test
    task_type: visual  # optional; can be a list like [rest, auditory]
    num_sessions: 72
    context_length_s: 61.44
    total_length_s: 296.96 # 4.95 minutes

  # STEP 2: Generate rollouts from sampled contexts.
  generator:
    enabled: true
    max_plot_seconds: 122.88
    seed: 42
    paired_file: paired_rollouts.npy
    rollouts_per_context: 1
    rollout_batch_size: 72
    max_context_tokens: 24576  # = 61.44 seconds
    kv_overlap: 4096  # = 10.24 seconds
    sampling:
      strategy: roulette    

  # STEP 3: Analyses (each uses its own YAML config).
  # analyses:
  #   - class: RolloutSlidingWindowAnalysis
  #     config: configs/flatgpt/analyses/sliding_window_main.yaml
  #   - class: RolloutDivergenceAnalysis
  #     config: configs/flatgpt/analyses/rollout_divergence_main.yaml

  token_summary:
    enabled: false
    tokens_per_second: 400
