trf_class: BLT
tok_class: DelimitedTokenizer
hidden_size: 1024
vocab_size: 257
input_shape: [1000, 68, 1]  # T, H, W

trf_args:
  patcher_config:
    hidden_size: 384
    intermediate_size: 1024
    num_hidden_layers: 8
    num_attention_heads: 6
    rope_theta: 40000.0
    max_position_embeddings: 32768
    vocab_size: 257

  encoder_config:
    vocab_size: 257
    hidden_size_global: 1024
    hidden_size: 512
    intermediate_size: 1408
    num_attention_heads: 8
    num_hidden_layers: 1
    max_position_embeddings: 98304  # this is base * 6
    rope_theta: 2000000.0

  decoder_config:
    vocab_size: 257
    hidden_size_global: 1024
    hidden_size: 512
    intermediate_size: 1408
    num_attention_heads: 8
    num_hidden_layers: 4
    max_position_embeddings: 98304
    rope_theta: 2000000.0

  global_config:
    hidden_size: 1024
    intermediate_size: 2816
    num_hidden_layers: 12
    num_attention_heads: 8
    max_position_embeddings: 16384
    rope_theta: 2000000.0


