save_dir: /vol/data/trainings/omega/1to50hz_ss/vqflatgpt/qwen2p5_flat
resume_from: /vol/data/trainings/omega/1to50hz_ss/vqflatgpt/qwen2p5_flat/logs/version_57/checkpoints/last-checkpoint-epoch00003.ckpt

model_name: FlatGPTEmbedsRVQ
loss_name: CrossEntropyWithCodes
model_config: configs/vqflatgpt/qwen2p5/model_sparse.yaml

datasplitter:
  dataset_class: ChunkDatasetReconstruction
  dataset_root: /vol/data/datasets/omega/full/1to50hz_ss_cont
  example_seconds: 57.6  # divisible by 1.28
  overlap_seconds: 5
  val_ratio: 0.1
  test_ratio: 0.1
  refresh_cache: false  # need to refresh if any datasplitter args change
  cache_dir: /vol/data/datasets/omega/full/cache/1to50hz_ss_cont_57.6

dataloader:
  batch_size: 6
  num_workers: 16
  prefetch_factor: 4
  pin_memory: true
  persistent_workers: true

lightning:
  lr: 2.0e-4
  weight_decay: 0.1  # 0.1 in qwen
  compile: true
  benchmark_train_step: false
  lr_scheduler:
    class_name: CosineAnnealingLR
    eta_min: 1.0e-5
    T_max: 100

trainer:
  tune_batch_size: false
  max_epochs: 100
  accelerator: cuda
  check_val_every_n_epoch: 1
  checkpoint_cadence_epochs: 1
  num_sanity_val_steps: 0
  log_every_n_steps: 100
  precision: bf16-mixed
  gradient_clip_val: 1.0  # 1.0 in qwen
  early_stopping:
    monitor: val/loss
    patience: 5

eval_runner:
  enabled: true
  max_batches: 100
  num_examples: 0
  generate:
    enabled: true
    num_runs: 1
    kv_overlap: 3200
    seconds: 115.2
    top_p: 1.0
    divergence_window_seconds: 10.0
    divergence_stride_seconds: 2.0
