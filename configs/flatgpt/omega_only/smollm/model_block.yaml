trf_class: SmoLLM3
tok_class: AmplitudeTokenizer
hidden_size: 192
vocab_size: 256
input_shape: [1000, 68, 1]  # T, H, W

trf_args:
  intermediate_size: 768
  num_hidden_layers: 16
  num_attention_heads: 8
  num_key_value_heads: 2
  attention_dropout: 0.0

  max_position_embeddings: 68000
  pad_token_id: null
  no_rope_layer_interval: 4
  rope_theta: 1000000.0

  _attn_implementation: flex_attention
  block_size: 68

  no_rope_layers: [
    1,
    1,
    1,
    0,
    1,
    1,
    1,
    0,
    1,
    1,
    1,
    0,
    1,
    1,
    1,
    0
  ]