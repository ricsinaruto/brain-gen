trf_class: MiniMax
hidden_size: 1024
vocab_size: 16384
input_shape: [5760, 68, 4]  # T, H, W
spatial_reduction: [8, 1]
temporal_reduction: 8

tokenizer_path: /vol/data/trainings/omega/1to50hz_ss/brainomni_tok/logs/version_11/checkpoints/best-checkpoint-epoch00082.ckpt

trf_args:
  intermediate_size: 1536
  num_hidden_layers: 8
  num_attention_heads: 8
  num_key_value_heads: 2
  head_dim: 128
  hidden_act: silu
  attention_dropout: 0.0
  
  _attn_implementation: sdpa
  use_cache: false

  num_experts_per_tok: 2
  num_local_experts: 4  # original: 32
  
  rope_theta: 1000000  # original: 10M
  rotary_dim: 64
  max_position_embeddings: 32768  # should be higher than T' x H' x W'
  


  initializer_range: 0.02
  layernorm_full_attention_alpha: 3.5565588200778455
  layernorm_full_attention_beta: 1.0
  layernorm_linear_attention_alpha: 3.5565588200778455
  layernorm_linear_attention_beta: 1.0
  layernorm_mlp_alpha: 3.5565588200778455
  layernorm_mlp_beta: 1.0
  output_router_logits: false
  postnorm: true
  rms_norm_eps: 1.0e-05
  router_aux_loss_coef: 0.001
  router_jitter_noise: 0.0
  shared_intermediate_size: 0
  shared_moe_mode: sigmoid

  attn_type_list: [0,0,0,0,0,0,0,1]
