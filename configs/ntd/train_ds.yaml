# Long-run NTD training on the multi-dataset (train_ds-style) split.
model_config: configs/ntd/model_ds.yaml

save_dir: /vol/data/trainings/all/ntd
resume_from: null

model_name: NTD
loss_name: NTDLoss

datasplitter:
  dataset_class: ChunkDatasetReconstruction
  dataset_root:
    mous: /vol/data/datasets/mous/full/1to50hz_ss_cont
    omega: /vol/data/datasets/omega/full/1to50hz_ss_cont2
    camcan: /vol/data/datasets/camcan/full/1to50hz_ss_cont
  example_seconds: 61.44  # 1024 samples at 100 Hz
  overlap_seconds: 30.0
  split_strategy: dataset
  heldout_dataset: mous
  val_ratio: 0.1
  test_ratio: 0.1
  refresh_cache: false  # refresh if datasplitter args change
  cache_dir: /vol/data/datasets/all/cache/1to50hz_ss_cont_61.44s_co-m

dataloader:
  batch_size: 16
  num_workers: 16
  prefetch_factor: 4
  pin_memory: true
  persistent_workers: true

lightning:
  lr: 2.0e-4
  weight_decay: 0.01
  compile: true
  lr_warmup:
    steps: 300      # counts scheduler steps (or epochs if interval is epoch)
    interval: step   # optional; defaults to scheduler interval
  lr_scheduler:
    interval: step
    class_name: CosineAnnealingLR
    eta_min: 2.0e-5
    T_max: 100

trainer:
  tune_batch_size: false
  max_epochs: 100
  accelerator: cuda
  check_val_every_n_epoch: 1
  checkpoint_cadence_epochs: 1
  log_every_n_steps: 100
  precision: bf16-mixed
  gradient_clip_val: 1.0
  early_stopping:
    monitor: val/loss
    patience: 5

eval_runner:
  enabled: true
  max_batches: 1000
  num_examples: 10

  example_sampler:
    seed: 42
    split: val
    num_sessions: 10
    context_length_s: 61.44
    total_length_s: 184.32

  generator:
    enabled: true
    rollouts_per_context: 1
    rollout_batch_size: 10
    kv_overlap: 128  # passed as sliding_window_overlap for NTD forecast
