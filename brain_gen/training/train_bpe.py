from __future__ import annotations

import os
from pathlib import Path
from collections import Counter
from typing import Iterable, Sequence

from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import CharDelimiterSplit
from transformers import PreTrainedTokenizerFast


class TextBPETokenizerTrainer:
    """Utility for training a Byte-Pair Encoding tokenizer on stage-3 text chunks.

    The corpus is assumed to be a directory containing `.txt` files generated by
    `Preprocessing.preprocess_stage_3` (space-separated tokens per chunk).
    """

    def __init__(
        self,
        text_dir: str,
        *,
        vocab_size: int = 32000,
        min_frequency: int = 2,
        special_tokens: Sequence[str] | None = None,
        limit_files: int | None = None,
        save_dir: str | None = None,
        split_spaces: bool = True,
        plot_distribution: bool = False,
        plot_distribution_top_k: int = 50,
        plot_distribution_max_texts: int | None = None,
        plot_distribution_max_tokens: int | None = None,
        plot_distribution_path: str | None = None,
    ) -> None:
        """Args:

        text_dir: Root directory containing `.txt` chunk files. vocab_size: Target
        vocabulary size for BPE. min_frequency: Minimum token frequency to be kept.
        special_tokens: Optional list of special tokens to reserve. limit_files:
        Optional limit on the number of files to read save_dir: Optional directory to
        save the tokenizer. split_spaces: Whether to split spaces into separate tokens.
        plot_distribution: Whether to plot token frequencies after training.
        plot_distribution_top_k: Number of most frequent tokens to include.
        plot_distribution_max_texts: Optional cap on how many text samples to tokenize
        when building the histogram. plot_distribution_max_tokens: Optional cap on total
        tokens to count. plot_distribution_path: Optional path to save the plot;
        defaults to     `<save_dir>/token_distribution.png` when save_dir is set.
        """
        self.text_dir = Path(text_dir)
        self.vocab_size = vocab_size
        self.min_frequency = min_frequency
        self.special_tokens = (
            list(special_tokens)
            if special_tokens is not None
            else ["[PAD]", "[UNK]", "[SEP]", "[BOS]", "[EOS]"]
        )
        self.limit_files = limit_files
        self.save_dir = save_dir
        self.split_spaces = split_spaces
        self.plot_distribution = plot_distribution
        self.plot_distribution_top_k = plot_distribution_top_k
        self.plot_distribution_max_texts = plot_distribution_max_texts
        self.plot_distribution_max_tokens = plot_distribution_max_tokens
        self.plot_distribution_path = (
            Path(plot_distribution_path) if plot_distribution_path else None
        )

    def _iter_files(self) -> Iterable[Path]:
        """Walk the directory tree and yield .txt files from nested subdirectories."""
        count = 0
        for root, _dirs, files in os.walk(self.text_dir):
            for name in sorted(files):
                if not name.endswith(".txt"):
                    continue
                yield Path(root) / name
                count += 1
                if self.limit_files is not None and count >= self.limit_files:
                    return

    def _iter_text(self) -> Iterable[str]:
        for path in self._iter_files():
            try:
                with open(path, "r", encoding="utf-8") as handle:
                    for line in handle:
                        text = line.strip()
                        if text:
                            yield text
            except Exception as exc:  # pragma: no cover - logged for visibility
                print(f"WARNING: Failed to read {path}: {exc}")

    def avg_token_string_len(self, tokenizer, texts, sample_tokens=2_000_000):
        total = 0
        count = 0
        for s in texts:
            ids = tokenizer.encode(s, add_special_tokens=False)
            toks = tokenizer.convert_ids_to_tokens(ids)
            for t in toks:
                total += len(t)
                count += 1
                if count >= sample_tokens:
                    break
            if count >= sample_tokens:
                break
        return total / max(count, 1), count

    def token_counts(
        self,
        tokenizer: PreTrainedTokenizerFast,
        corpus_iter: Iterable[str],
        *,
        max_texts: int | None = None,
        max_tokens: int | None = None,
    ) -> tuple[Counter[int], int, int]:
        """Tokenize corpus_iter and return token-id counts plus basic metadata.

        Args:     tokenizer: The tokenizer used to encode the text.     corpus_iter:
        Iterable of raw text strings.     max_texts: Optional cap on the number of text
        rows to process.     max_tokens: Optional cap on the number of tokens to count.

        Returns:     counts: Counter mapping token id -> frequency.     num_texts:
        Number of text rows consumed.     num_tokens: Number of tokens counted.
        """

        counts: Counter[int] = Counter()
        num_texts = 0
        num_tokens = 0

        for text in corpus_iter:
            ids = tokenizer.encode(text, add_special_tokens=False)
            counts.update(ids)

            num_texts += 1
            num_tokens += len(ids)

            if max_texts is not None and num_texts >= max_texts:
                break
            if max_tokens is not None and num_tokens >= max_tokens:
                break

        return counts, num_texts, num_tokens

    def plot_token_distribution(
        self,
        tokenizer: PreTrainedTokenizerFast,
        corpus_iter: Iterable[str],
        *,
        top_k: int = 50,
        max_texts: int | None = None,
        max_tokens: int | None = None,
        save_path: str | Path | None = None,
    ) -> None:
        """Plot a histogram of the most frequent tokens in the provided text."""

        import matplotlib.pyplot as plt

        counts, num_texts, num_tokens = self.token_counts(
            tokenizer,
            corpus_iter,
            max_texts=max_texts,
            max_tokens=max_tokens,
        )

        if not counts:
            print("Token distribution: corpus iterator produced no tokens.")
            return

        most_common = counts.most_common(top_k)
        token_ids = [tok_id for tok_id, _ in most_common]
        freqs = [freq for _, freq in most_common]
        token_strings = tokenizer.convert_ids_to_tokens(token_ids)

        target_path: Path | None
        if save_path is not None:
            target_path = Path(save_path)
        elif self.save_dir is not None:
            target_path = Path(self.save_dir) / "token_distribution.png"
        else:
            target_path = Path.cwd() / "token_distribution.png"

        target_path.parent.mkdir(parents=True, exist_ok=True)

        plt.figure(figsize=(12.0, 6.0))
        ax = plt.gca()
        ax.bar(range(len(freqs)), freqs, width=1.0)
        ax.set_xlim(-0.5, len(freqs) - 0.5)

        max_labels = 120  # keep axis readable when plotting thousands of tokens
        if len(token_strings) <= max_labels:
            ax.set_xticks(range(len(token_strings)))
            ax.set_xticklabels(token_strings, rotation=90, fontsize=6)
        else:
            step = max(1, len(token_strings) // max_labels)
            tick_positions = list(range(0, len(token_strings), step))
            ax.set_xticks(tick_positions)
            ax.set_xticklabels(
                [token_strings[i] for i in tick_positions],
                rotation=90,
                fontsize=6,
            )

        ax.set_xlabel("Token")
        ax.set_ylabel("Count")
        ax.set_title(
            f"Top {len(token_ids)} tokens (texts={num_texts}, "
            f"tokens={num_tokens:,})"
        )
        plt.tight_layout()
        plt.savefig(target_path)
        plt.close()
        print(f"Token distribution plot saved to {target_path}")

    def train(self) -> PreTrainedTokenizerFast:
        """Train a ByteLevel BPE tokenizer and optionally persist it.

        Returns:     A `PreTrainedTokenizerFast` instance ready for downstream use.
        """
        if not self.text_dir.exists():
            raise FileNotFoundError(f"Text directory not found: {self.text_dir}")

        tokenizer = Tokenizer(BPE(unk_token="[UNK]", byte_fallback=True))
        tokenizer.pre_tokenizer = CharDelimiterSplit(" ") if self.split_spaces else None

        trainer = BpeTrainer(
            vocab_size=self.vocab_size,
            min_frequency=self.min_frequency,
            special_tokens=self.special_tokens,
            show_progress=True,
        )
        corpus_iter = self._iter_text()
        tokenizer.train_from_iterator(corpus_iter, trainer=trainer)

        tokenizer = PreTrainedTokenizerFast(
            tokenizer_object=tokenizer,
            bos_token="[BOS]" if "[BOS]" in self.special_tokens else None,
            eos_token="[EOS]" if "[EOS]" in self.special_tokens else None,
            unk_token="[UNK]" if "[UNK]" in self.special_tokens else None,
            pad_token="[PAD]" if "[PAD]" in self.special_tokens else None,
        )

        if self.save_dir is not None:
            os.makedirs(self.save_dir, exist_ok=True)
            tokenizer.save_pretrained(self.save_dir)

        corpus_iter = self._iter_text()
        avg, n = self.avg_token_string_len(tokenizer, corpus_iter)
        print(f"avg token *string* length = {avg:.2f} (tokens={n:,})")

        if self.plot_distribution:
            corpus_iter = self._iter_text()
            self.plot_token_distribution(
                tokenizer,
                corpus_iter,
                top_k=self.plot_distribution_top_k,
                max_texts=self.plot_distribution_max_texts,
                max_tokens=self.plot_distribution_max_tokens,
                save_path=self.plot_distribution_path,
            )

        return tokenizer
