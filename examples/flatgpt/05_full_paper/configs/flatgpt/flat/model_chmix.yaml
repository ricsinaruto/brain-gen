trf_class: Qwen2_5_Video
hidden_size: 1200
vocab_size: 16384
input_shape: [6144, 68, 4]  # T, H, W
spatial_reduction: [16, 1]
temporal_reduction: 4

tokenizer_path: /path/to/tokenizers/braintokmix/checkpoints/best-checkpoint.ckpt

token_corruption_cfg:
  enabled: true
  p_end: 0.02
  block_len_steps: 16

trf_args:
  intermediate_size: 4560
  num_hidden_layers: 12  # 36 in 3B model
  head_dim: 120
  num_attention_heads: 10
  num_key_value_heads: 2
  attention_dropout: 0.0
  
  max_position_embeddings: 32768  # should be higher than T' x H' x W'
  rope_theta: 1000000.0
  rope_scaling:
    mrope_section: 20
    rope_type: default

  _attn_implementation: sdpa
