# Scaled-down tokenizer training config for small local data.

save_dir: tmp/examples/flatgpt/tokenizer
resume_from: null

model_name: BrainOmniCausalTokenizerSEANetChannelMix
loss_name: BrainOmniCausalTokenizerLoss
model_config: examples/flatgpt/02_tokenizer/configs/braintokmix_model_small.yaml

datasplitter:
  dataset_class: ChunkDatasetReconstruction
  dataset_root:
    omega: data/omega/small/1to50hz_ss_cont
    camcan: data/camcan/small/1to50hz_ss_cont
    mous: data/mous/small/1to50hz_ss_cont
  example_seconds: 10.24
  overlap_seconds: 5.0
  split_strategy: dataset
  heldout_dataset: mous
  refresh_cache: true
  cache_dir: tmp/examples/flatgpt/cache/tokenizer_10p24s

dataloader:
  batch_size: 128
  num_workers: 8
  prefetch_factor: 2
  pin_memory: true
  persistent_workers: true
  drop_last: true

lightning:
  lr: 5.0e-4
  weight_decay: 0.01
  compile: false
  lr_warmup:
    steps: 10
    interval: step

trainer:
  tune_batch_size: false
  accelerator: cpu
  precision: 32
  gradient_clip_val: 1.0

  max_epochs: 10
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 0
  log_every_n_steps: 10

  early_stopping:
    monitor: val/loss
    patience: 5

eval_runner:
  enabled: false
